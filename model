#RandomForests için uygun parametreleri bulmak adına ;

from sklearn.model_selection import GridSearchCV

rf = RandomForestClassifier(random_state=46)

param_grid = {
    'n_estimators': [100, 200, 300],
    'max_depth': [None, 10, 20, 30],
    'min_samples_split': [2, 5, 10],
    'min_samples_leaf': [1, 2, 4]
}

grid_search = GridSearchCV(estimator=rf, param_grid=param_grid,
                           cv=5, n_jobs=-1, scoring='accuracy')
grid_search.fit(X_train, y_train)

print("Best parameters:", grid_search.best_params_)
print("Best accuracy:", grid_search.best_score_)

##################################################################################
Best parameters: {'max_depth': None, 'min_samples_leaf': 2, 'min_samples_split': 10, 'n_estimators': 200}
Best accuracy: 0.8438172043010752
#################################################################################
from sklearn.ensemble import RandomForestClassifier
from sklearn.metrics import precision_score, recall_score, f1_score, roc_auc_score
from sklearn.metrics import confusion_matrix
import seaborn as sns
import matplotlib.pyplot as plt


y = df["Depression_1"]
X = df.drop("Depression_1", axis=1)
X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.20, random_state=17)


#rf_model = RandomForestClassifier(random_state=46).fit(X_train, y_train)
#y_pred = rf_model.predict(X_test)


rf_model = RandomForestClassifier(
    n_estimators=300,
    max_depth=None,
    min_samples_leaf=1,
    min_samples_split=2,
    random_state=46
)

rf_model.fit(X_train, y_train)


print(f"Accuracy: {round(accuracy_score(y_pred, y_test), 2)}")
print(f"Recall: {round(recall_score(y_pred,y_test),3)}")
print(f"Precision: {round(precision_score(y_pred,y_test), 2)}")
print(f"F1: {round(f1_score(y_pred,y_test), 2)}")
print(f"Auc: {round(roc_auc_score(y_pred,y_test), 2)}")

cm = confusion_matrix(testY, y_pred)
plt.figure(figsize=(6, 4))
sns.heatmap(cm, annot=True, fmt='d', cmap='Blues')
plt.xlabel('Tahmin')
plt.ylabel('Gerçek')
plt.title('Confusion Matrix')
plt.show()
#############################################################################################
Accuracy: 0.84
Recall: 0.846
Precision: 0.88
F1: 0.86
Auc: 0.84
############################################################################################
#DNN;
X = df.drop('Depression', axis=1)
y = df['Depression']

from sklearn.model_selection import train_test_split

trainX, testX, trainY, testY = train_test_split(
    X, y, 
    test_size=0.2, 
    random_state=42,   
    #stratify=y         
)

sc=StandardScaler()

scaler = sc.fit(trainX)
trainX_scaled = scaler.transform(trainX)
testX_scaled = scaler.transform(testX)

mlp = MLPClassifier(
    hidden_layer_sizes=(128,128,64),  
    activation='relu',
    solver='adam',
    learning_rate_init=0.001,
    alpha=0.0001,  # L2 regularization
    batch_size=256,
    early_stopping=True,
    validation_fraction=0.1,
    max_iter=500,
    random_state=42
) 
mlp.fit(trainX, trainY)
y_pred = mlp.predict(testX)
print('Test Doğruluk (Accuracy): {:.2f}'.format(accuracy_score(testY, y_pred)))


cm = confusion_matrix(testY, y_pred)
plt.figure(figsize=(6, 4))
sns.heatmap(cm, annot=True, fmt='d', cmap='Reds')
plt.xlabel('Tahmin')
plt.ylabel('Gerçek')
plt.title('Confusion Matrix')
plt.show()

model = Sequential([
    Dense(512, activation='relu', input_shape=(trainX.shape[1],)),
    Dropout(0.4),
    Dense(256, activation='relu'),
    Dropout(0.4),
    Dense(256, activation='relu'),
    Dropout(0.4),
    Dense(128, activation='relu'),
    Dropout(0.3),
    Dense(1, activation='sigmoid')])

model.compile(
    optimizer=Adam(learning_rate=0.0001),
    loss='binary_crossentropy',
    metrics=['accuracy'])

early_stop = EarlyStopping(
    monitor='val_loss',
    patience=10,
    restore_best_weights=True)

history = model.fit(
    trainX, trainY,
    validation_split=0.2,
    epochs=150,
    batch_size=256,
    callbacks=[early_stop],
    verbose=1)

y_pred_prob = model.predict(testX)
y_pred = (y_pred_prob > 0.5).astype(int)

print("Accuracy: {:.4f}".format(accuracy_score(testY, y_pred)))
print("\nClassification Report:\n", classification_report(testY, y_pred))
##########################################################################
Classification Report:
               precision    recall  f1-score   support

       False       0.82      0.79      0.81      2343
        True       0.85      0.87      0.86      3238

    accuracy                           0.84      5581
   macro avg       0.84      0.83      0.83      5581
weighted avg       0.84      0.84      0.84      5581
##########################################################################
SVM;

