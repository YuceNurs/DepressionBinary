#RandomForests için uygun parametreleri bulmak adına ;

from sklearn.model_selection import GridSearchCV

rf = RandomForestClassifier(random_state=46)

param_grid = {
    'n_estimators': [100, 200, 300],
    'max_depth': [None, 10, 20, 30],
    'min_samples_split': [2, 5, 10],
    'min_samples_leaf': [1, 2, 4]
}

grid_search = GridSearchCV(estimator=rf, param_grid=param_grid,
                           cv=5, n_jobs=-1, scoring='accuracy')
grid_search.fit(X_train, y_train)

print("Best parameters:", grid_search.best_params_)
print("Best accuracy:", grid_search.best_score_)

##################################################################################
Best parameters: {'max_depth': None, 'min_samples_leaf': 2, 'min_samples_split': 10, 'n_estimators': 200}
Best accuracy: 0.8438172043010752
#################################################################################
from sklearn.ensemble import RandomForestClassifier
from sklearn.metrics import precision_score, recall_score, f1_score, roc_auc_score
from sklearn.metrics import confusion_matrix
import seaborn as sns
import matplotlib.pyplot as plt


y = df["Depression_1"]
X = df.drop("Depression_1", axis=1)
X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.20, random_state=17)


#rf_model = RandomForestClassifier(random_state=46).fit(X_train, y_train)
#y_pred = rf_model.predict(X_test)


rf_model = RandomForestClassifier(
    n_estimators=300,
    max_depth=None,
    min_samples_leaf=1,
    min_samples_split=2,
    random_state=46
)

rf_model.fit(X_train, y_train)


print(f"Accuracy: {round(accuracy_score(y_pred, y_test), 2)}")
print(f"Recall: {round(recall_score(y_pred,y_test),3)}")
print(f"Precision: {round(precision_score(y_pred,y_test), 2)}")
print(f"F1: {round(f1_score(y_pred,y_test), 2)}")
print(f"Auc: {round(roc_auc_score(y_pred,y_test), 2)}")

cm = confusion_matrix(testY, y_pred)
plt.figure(figsize=(6, 4))
sns.heatmap(cm, annot=True, fmt='d', cmap='Blues')
plt.xlabel('Tahmin')
plt.ylabel('Gerçek')
plt.title('Confusion Matrix')
plt.show()
#############################################################################################
Accuracy: 0.84
Recall: 0.846
Precision: 0.88
F1: 0.86
Auc: 0.84
############################################################################################
#DNN;
X = df.drop('Depression', axis=1)
y = df['Depression']

from sklearn.model_selection import train_test_split

trainX, testX, trainY, testY = train_test_split(
    X, y, 
    test_size=0.2, 
    random_state=42,   
    #stratify=y         
)

sc=StandardScaler()

scaler = sc.fit(trainX)
trainX_scaled = scaler.transform(trainX)
testX_scaled = scaler.transform(testX)

mlp = MLPClassifier(
    hidden_layer_sizes=(128,128,64),  
    activation='relu',
    solver='adam',
    learning_rate_init=0.001,
    alpha=0.0001,  # L2 regularization
    batch_size=256,
    early_stopping=True,
    validation_fraction=0.1,
    max_iter=500,
    random_state=42
) 
mlp.fit(trainX, trainY)
y_pred = mlp.predict(testX)
print('Test Doğruluk (Accuracy): {:.2f}'.format(accuracy_score(testY, y_pred)))


cm = confusion_matrix(testY, y_pred)
plt.figure(figsize=(6, 4))
sns.heatmap(cm, annot=True, fmt='d', cmap='Reds')
plt.xlabel('Tahmin')
plt.ylabel('Gerçek')
plt.title('Confusion Matrix')
plt.show()

model = Sequential([
    Dense(512, activation='relu', input_shape=(trainX.shape[1],)),
    Dropout(0.4),
    Dense(256, activation='relu'),
    Dropout(0.4),
    Dense(256, activation='relu'),
    Dropout(0.4),
    Dense(128, activation='relu'),
    Dropout(0.3),
    Dense(1, activation='sigmoid')])

model.compile(
    optimizer=Adam(learning_rate=0.0001),
    loss='binary_crossentropy',
    metrics=['accuracy'])

early_stop = EarlyStopping(
    monitor='val_loss',
    patience=10,
    restore_best_weights=True)

history = model.fit(
    trainX, trainY,
    validation_split=0.2,
    epochs=150,
    batch_size=256,
    callbacks=[early_stop],
    verbose=1)

y_pred_prob = model.predict(testX)
y_pred = (y_pred_prob > 0.5).astype(int)

print("Accuracy: {:.4f}".format(accuracy_score(testY, y_pred)))
print("\nClassification Report:\n", classification_report(testY, y_pred))
##########################################################################
Classification Report:
               precision    recall  f1-score   support

       False       0.82      0.79      0.81      2343
        True       0.85      0.87      0.86      3238

    accuracy                           0.84      5581
   macro avg       0.84      0.83      0.83      5581
weighted avg       0.84      0.84      0.84      5581
##########################################################################
SVM;

param_grid = {
    'svm__C': [0.1, 1, 10],
    'svm__gamma': ['scale', 0.01, 0.001],
    'svm__kernel': ['rbf', 'linear']
}

svm_pipeline = Pipeline([
    ("scaler", StandardScaler()),
    ("svm", SVC(probability=True, random_state=42))
])

grid = GridSearchCV(svm_pipeline, param_grid, cv=5, scoring='accuracy', n_jobs=-1)
grid.fit(X_train, y_train)

print("Best Params:", grid.best_params_)
print("Best Accuracy:", grid.best_score_)
#############
from sklearn.preprocessing import StandardScaler
from sklearn.pipeline import Pipeline

param_grid = {
    'svm__C':0.1,
    'svm__gamma': 0.01,
    'svm__kernel': 'linear'
}


svm_pipeline = Pipeline([
    ("scaler", StandardScaler()),
    ("svm", SVC(kernel="rbf", C=1.0, gamma="scale", probability=True, random_state=42))
])


# Eğit ve tahmin et
svm_pipeline.fit(trainX, trainY)
y_pred = svm_pipeline.predict(testX)
y_proba = svm_pipeline.predict_proba(testX)[:, 1]

# Metrikler
print(f"Accuracy: {round(accuracy_score(testY, y_pred), 2)}")
print(f"Recall: {round(recall_score(testY, y_pred), 3)}")
print(f"Precision: {round(precision_score(testY, y_pred), 2)}")
print(f"F1: {round(f1_score(testY, y_pred), 2)}")
print(f"AUC: {round(roc_auc_score(testY, y_proba), 2)}")

###################################################################
Accuracy: 0.83
Recall: 0.879
Precision: 0.84
F1: 0.86
AUC: 0.91
#####################################################################
#Naive Bayes
# Naive Bayes
from sklearn.metrics import precision_score
from sklearn.metrics import recall_score
from sklearn.metrics import f1_score


nb_model = GaussianNB()

nb_model.fit(trainX, trainY)

predY = nb_model.predict(testX)


accuracy = accuracy_score(testY, predY)
precision = precision_score(testY, predY)
recall = recall_score(testY, predY)
f1 = f1_score(testY, predY)
classification_rep = classification_report(testY, predY)


print(f"Accuracy: {accuracy}")
print(f"Precision: {precision}")
print(f"Recall: {recall}")
print(f"F1 Score: {f1}")
print("\nClassification Report:\n")
print(classification_rep)


conf_matrix = confusion_matrix(y_test, y_pred)


plt.figure(figsize=(8, 6))
sns.heatmap(conf_matrix, annot=True, fmt='d', cmap='Blues', xticklabels=nb_model.classes_, yticklabels=nb_model.classes_)
plt.title('Confusion Matrix for Naive Bayes')
plt.xlabel('Predicted')
plt.ylabel('Actual')
plt.show()
####################################################################
Accuracy: 0.766547192353644
Precision: 0.8083228247162674
Recall: 0.7868248772504092
F1 Score: 0.7974289861082314

Classification Report:

              precision    recall  f1-score   support

           0       0.71      0.74      0.72      3482
           1       0.81      0.79      0.80      4888

    accuracy                           0.77      8370
   macro avg       0.76      0.76      0.76      8370
weighted avg       0.77      0.77      0.77      8370
#######################################################################
# logisitic regression model
from sklearn.linear_model import LogisticRegression

logistic_model = LogisticRegression()
logistic_model.fit(X_train_scaled, trainY)

predY = logistic_model.predict(X_test_scaled)

accuracy = accuracy_score(testY,predY)
precision = precision_score(testY, predY, average='weighted')  
recall = recall_score(testY, predY, average='weighted')
f1 = f1_score(testY, predY, average='weighted')
classification_rep = classification_report(testY, predY)

conf_matrix = confusion_matrix(testY, predY)

print(f"Accuracy: {accuracy}")
print(f"Precision: {precision}")
print(f"Recall: {recall}")
print(f"F1 Score: {f1}")
print("\nClassification Report:\n")
print(classification_rep)

disp = ConfusionMatrixDisplay(confusion_matrix=conf_matrix, display_labels=logistic_model.classes_)
disp.plot(cmap=plt.cm.Blues)
plt.title("Confusion Matrix")
plt.show()
##############################################################
Accuracy: 0.8389177566744311
Precision: 0.8384120197864107
Recall: 0.8389177566744311
F1 Score: 0.8384155670806575

Classification Report:

              precision    recall  f1-score   support

       False       0.82      0.79      0.80      2343
        True       0.85      0.88      0.86      3238

    accuracy                           0.84      5581
   macro avg       0.84      0.83      0.83      5581
weighted avg       0.84      0.84      0.84      5581
#################################################################
