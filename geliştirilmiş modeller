import warnings
import numpy as np
import pandas as pd
import seaborn as sns
from matplotlib import pyplot as plt
from sklearn.ensemble import RandomForestClassifier, GradientBoostingClassifier, VotingClassifier
from sklearn.model_selection import GridSearchCV, cross_validate, RandomizedSearchCV, validation_curve
from xgboost import XGBClassifier
from lightgbm import LGBMClassifier
from catboost import CatBoostClassifier
#XGBoost : GBM'in hız ve tahmin performansını arttıran framework

xgboost_model= XGBClassifier(random_state=17)

cv_results=cross_validate(xgboost_model, X,y,cv=5, scoring=["accuracy","f1","roc_auc"])
cv_results['test_accuracy'].mean()
cv_results['test_f1'].mean()
cv_results["test_roc_auc"].mean()

xgboost_params={"learning_rate":[0.1,0.01,0.001],
              "max_depth":[5,8,12,15,20],
              "n_estimators":[100,500,1000],
              "subsample":[0.5,0.7,1]}

#LightGBM: XGboosttan daha iyisi/hızlı
#level-wise yerine leaf-wise büyüme stratejisi
#XGBoost geniş kapsamlı arama, LGBM derin arama

lgbm_model=  LGBMClassifier(random_state=17)
lgbm_model.get_params()

cv_results=cross_validate(lgbm_model, X,y,cv=5, scoring=["accuracy","f1","roc_auc"])
cv_results['test_accuracy'].mean()
cv_results['test_f1'].mean()
cv_results["test_roc_auc"].mean()

lgbm_params={"learning_rate":[0.1,0.01,0.001],
              "n_estimators":[100,500,1000],
              "subsample":[0.5,0.7,1]}

lgbm_best_grid =GridSearchCV(lgbm_model, lgbm_params, cv=5, n_jobs=-1, verbose=True).fit(X,y)

lgbm_final=lgbm_model.set_params(**lgbm_best_grid.best_params_,random_state=17).fit(X,y)
lgbm_params={"learning_rate":[0.01,0.02,0.05,0.1],
              "n_estimators":[200,300,350,400,1000,5000,10000],
              "colsample":[0.8,0.9,1]}

cv_results=cross_validate(lgbm_final, X,y,cv=5, scoring=["accuracy","f1","roc_auc"])
cv_results['test_accuracy'].mean()
cv_results['test_f1'].mean()
cv_results["test_roc_auc"].mean()

#Catboost(Category Boosting):hızlı

catboost_model =CatBoostClassifier(random_state=17,verbose=False)

cv_results= cross_validate(catboost_model, X,y,cv=5, scoring=["accuracy","f1","roc_auc"])
cv_results['test_accuracy'].mean()
cv_results['test_f1'].mean()
cv_results["test_roc_auc"].mean()

catboost_params= {"iterations":[200,500],
                  "learning_rate":[0.01,0.1],
                  "depth":[3,6]}

catboost_best_grid=GridSearchCV(catboost_model, catboost_params, cv=5, n_jobs=-1, verbose=True).fit(X,y)

catboost_final=catboost_model.set_params(**catboost_best_grid.best_params_,random_state=17).fit(X,y)

cv_results=cross_validate(catboost_final, X,y,cv=5, scoring=["accuracy","f1","roc_auc"])
cv_results['test_accuracy'].mean()
cv_results['test_f1'].mean()
cv_results["test_roc_auc"].mean()
##########################################################################################################33
[LightGBM] [Warning] Found whitespace in feature_names, replace with underlines
[LightGBM] [Info] Number of positive: 13068, number of negative: 9252
[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.000900 seconds.
You can set `force_row_wise=true` to remove the overhead.
And if memory is not enough, you can set `force_col_wise=true`.
[LightGBM] [Info] Total Bins 647
[LightGBM] [Info] Number of data points in the train set: 22320, number of used features: 29
[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.585484 -> initscore=0.345327
[LightGBM] [Info] Start training from score 0.345327
[LightGBM] [Warning] Found whitespace in feature_names, replace with underlines
[LightGBM] [Info] Number of positive: 13069, number of negative: 9252
[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.001384 seconds.
You can set `force_row_wise=true` to remove the overhead.
And if memory is not enough, you can set `force_col_wise=true`.
[LightGBM] [Info] Total Bins 647
[LightGBM] [Info] Number of data points in the train set: 22321, number of used features: 29
[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.585502 -> initscore=0.345403
[LightGBM] [Info] Start training from score 0.345403
[LightGBM] [Warning] Found whitespace in feature_names, replace with underlines
[LightGBM] [Info] Number of positive: 13069, number of negative: 9252
[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.001187 seconds.
You can set `force_row_wise=true` to remove the overhead.
And if memory is not enough, you can set `force_col_wise=true`.
[LightGBM] [Info] Total Bins 650
[LightGBM] [Info] Number of data points in the train set: 22321, number of used features: 29
[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.585502 -> initscore=0.345403
[LightGBM] [Info] Start training from score 0.345403
[LightGBM] [Warning] Found whitespace in feature_names, replace with underlines
[LightGBM] [Info] Number of positive: 13069, number of negative: 9252
[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.001284 seconds.
You can set `force_row_wise=true` to remove the overhead.
And if memory is not enough, you can set `force_col_wise=true`.
[LightGBM] [Info] Total Bins 651
[LightGBM] [Info] Number of data points in the train set: 22321, number of used features: 29
[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.585502 -> initscore=0.345403
[LightGBM] [Info] Start training from score 0.345403
[LightGBM] [Warning] Found whitespace in feature_names, replace with underlines
[LightGBM] [Info] Number of positive: 13069, number of negative: 9252
[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.000806 seconds.
You can set `force_row_wise=true` to remove the overhead.
And if memory is not enough, you can set `force_col_wise=true`.
[LightGBM] [Info] Total Bins 652
[LightGBM] [Info] Number of data points in the train set: 22321, number of used features: 29
[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.585502 -> initscore=0.345403
[LightGBM] [Info] Start training from score 0.345403
Fitting 5 folds for each of 27 candidates, totalling 135 fits
[LightGBM] [Warning] Found whitespace in feature_names, replace with underlines
[LightGBM] [Info] Number of positive: 16336, number of negative: 11565
[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.001342 seconds.
You can set `force_row_wise=true` to remove the overhead.
And if memory is not enough, you can set `force_col_wise=true`.
[LightGBM] [Info] Total Bins 653
[LightGBM] [Info] Number of data points in the train set: 27901, number of used features: 29
[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.585499 -> initscore=0.345388
[LightGBM] [Info] Start training from score 0.345388
[LightGBM] [Warning] Found whitespace in feature_names, replace with underlines
[LightGBM] [Info] Number of positive: 16336, number of negative: 11565
[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.001459 seconds.
You can set `force_row_wise=true` to remove the overhead.
And if memory is not enough, you can set `force_col_wise=true`.
[LightGBM] [Info] Total Bins 653
[LightGBM] [Info] Number of data points in the train set: 27901, number of used features: 29
[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.585499 -> initscore=0.345388
[LightGBM] [Info] Start training from score 0.345388
[LightGBM] [Warning] Found whitespace in feature_names, replace with underlines
[LightGBM] [Info] Number of positive: 13068, number of negative: 9252
[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.001177 seconds.
You can set `force_row_wise=true` to remove the overhead.
And if memory is not enough, you can set `force_col_wise=true`.
[LightGBM] [Info] Total Bins 647
[LightGBM] [Info] Number of data points in the train set: 22320, number of used features: 29
[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.585484 -> initscore=0.345327
[LightGBM] [Info] Start training from score 0.345327
[LightGBM] [Warning] Found whitespace in feature_names, replace with underlines
[LightGBM] [Info] Number of positive: 13069, number of negative: 9252
[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.001396 seconds.
You can set `force_row_wise=true` to remove the overhead.
And if memory is not enough, you can set `force_col_wise=true`.
[LightGBM] [Info] Total Bins 647
[LightGBM] [Info] Number of data points in the train set: 22321, number of used features: 29
[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.585502 -> initscore=0.345403
[LightGBM] [Info] Start training from score 0.345403
[LightGBM] [Warning] Found whitespace in feature_names, replace with underlines
[LightGBM] [Info] Number of positive: 13069, number of negative: 9252
[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.010781 seconds.
You can set `force_col_wise=true` to remove the overhead.
[LightGBM] [Info] Total Bins 650
[LightGBM] [Info] Number of data points in the train set: 22321, number of used features: 29
[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.585502 -> initscore=0.345403
[LightGBM] [Info] Start training from score 0.345403
[LightGBM] [Warning] Found whitespace in feature_names, replace with underlines
[LightGBM] [Info] Number of positive: 13069, number of negative: 9252
[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.001115 seconds.
You can set `force_row_wise=true` to remove the overhead.
And if memory is not enough, you can set `force_col_wise=true`.
[LightGBM] [Info] Total Bins 651
[LightGBM] [Info] Number of data points in the train set: 22321, number of used features: 29
[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.585502 -> initscore=0.345403
[LightGBM] [Info] Start training from score 0.345403
[LightGBM] [Warning] Found whitespace in feature_names, replace with underlines
[LightGBM] [Info] Number of positive: 13069, number of negative: 9252
[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.001324 seconds.
You can set `force_row_wise=true` to remove the overhead.
And if memory is not enough, you can set `force_col_wise=true`.
[LightGBM] [Info] Total Bins 652
[LightGBM] [Info] Number of data points in the train set: 22321, number of used features: 29
[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.585502 -> initscore=0.345403
[LightGBM] [Info] Start training from score 0.345403
Fitting 5 folds for each of 8 candidates, totalling 40 fits
np.float64(0.920338400691193)
###################################################################################################3
